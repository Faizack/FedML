inference_image_name: "fedml/fedml-default-inference-backend" #Optional, default
using_triton: False #Optional, default is False
inference_token: "my_token"
use_gpu: False

source_folder: "."
entry_point: "main_entry.py"
config_folder: "./fedml_model_config.yaml"

### This absolute direction is for local model files
### This folder will NOT be upload to MLOps,
### the inference server (Docker) will mount from local directory
data_cache_dir: "~/fedml_serving/model_and_config"

# If you want to install some packages
# Please write the command in the bootstrap.sh
bootstrap: config/bootstrap.sh