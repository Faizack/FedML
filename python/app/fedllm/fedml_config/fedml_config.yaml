common_args:
  training_type: "cross_silo"
  scenario: "horizontal"
  random_seed: 0

data_args:
  dataset: "databricks-dollys"  # dataset name
  data_cache_dir: ~/fedml_data
  # below copied from scripts/train_deepspeed.sh
  dataset_path:
    - ".data/databricks-dolly-15k.jsonl"
  test_dataset_size: 200

model_args:
  # below copied from scripts/train_deepspeed.sh
  model_name: "EleutherAI/pythia-70m" # EleutherAI/pythia-70m; EleutherAI/pythia-6.9b
  use_lora: True

train_args:
  federated_optimizer: "FedAvg"
  client_optimizer: sgd
  server_optimizer: "FedAvg"
  client_id_list:
  client_num_in_total: 2  # set number of clients
  client_num_per_round: 2  # choose from 1~client_num_in_total
  comm_round: 50
  # below copied from scripts/train_deepspeed.sh
  deepspeed: "configs/ds_z3_bf16_config.json"
  seed: 1234
  fp16: False
  bf16: False
  gradient_checkpointing: True
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  learning_rate: 0.000005
  warmup_steps: 50
  num_train_epochs: 1   # number of training epoch for client on each round
  output_dir: ".logs/FedML/dolly_pythia-70m"
  logging_steps: 50
  eval_steps: 200
  save_steps: 200
  max_steps: 4
  save_total_limit: 20
  logging_strategy: "no"
  evaluation_strategy: "no"  # should be turned off
  save_strategy: "no"
  eval_accumulation_steps: 4
#  save_on_each_node: True

validation_args:
  frequency_of_the_test: 5

device_args:
  using_gpu: True

comm_args:
  backend: "MQTT_S3"
  is_mobile: 0

tracking_args:
  enable_wandb: False
  wandb_only_server: True
